{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is for the major project submission for COMP8220, on the language dataset. The notebooks contains the following sections which we have used to obtain the required results\n",
    "\n",
    "1) We used 3 Conventional Models to find the accuracy of the language dataset and out of which the Random Forest model gave us the best accuracy. The other 2 models which were used were Logistic Regression and Naive Bayes model.\n",
    "* Logistic Regression Model : Logistic regression is fast and relatively uncomplicated fundamental classification technique.\n",
    "* Naive Bayes : As the dataset has three classes of the output from choose, we have used Multinomial Naive Bayes. We used this model as it is specially used for text documents. Its model is smaller than random forest and is faster.\n",
    "* Random Forest : Random forest consists of a large number of individual decision trees that operate as an ensemble and each individual tree in the random forest gives out a class prediction and the class with the most votes becomes the model’s prediction. It is robust against overfitting at and gives better results with more samples.\n",
    "\n",
    "2) Convolutional Neural Network: Convolutional neural network is a class of deep neural networks. It is a neural Network that has one or more convolutional layers and is used for classification, segmentation and other auto correlated data. It is an algorithm which takes and input, assigns weights and importance to the objects in the data.\n",
    "\n",
    "3) Performance : Random forest gave the highest accuracy was higher for the validation dataset as compared to the CNN model, but the CNN model had higher accuracy for both the public and private dataset on kaggle. The CNN model accuracy for the public dataset was 0.56865 while for the private dataset was 0.64129 and had a significant increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and downloading all the required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from textblob import Word\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.layers import Dense, Dropout\n",
    "import string\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded C:/Users/abhis/Downloads/tweet-emotion-detection/language_dataset/text_word_to_idx.pkl..\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "import pickle\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        file = pickle.load(f)\n",
    "        print ('Loaded %s..' %path)\n",
    "        return file\n",
    "\n",
    "dataset_directory = 'C:/Users/abhis/Downloads/tweet-emotion-detection/language_dataset/'\n",
    "private_directory = 'C:/Users/abhis/Downloads/tweet-emotion-detection-private/'\n",
    "emotions = ['anger', 'fear', 'joy', 'sadness']\n",
    "\n",
    "tweets_train = np.load(join(dataset_directory, 'text_train_tweets.npy'))\n",
    "labels_train = np.load(join(dataset_directory, 'text_train_labels.npy'))\n",
    "vocabulary = load_pickle(join(dataset_directory, 'text_word_to_idx.pkl'))\n",
    "\n",
    "tweets_val = np.load(join(dataset_directory, 'text_val_tweets.npy'))\n",
    "labels_val = np.load(join(dataset_directory, 'text_val_labels.npy'))\n",
    "\n",
    "tweets_test_public = np.load(join(dataset_directory, 'text_test_public_tweets_rand.npy'))\n",
    "tweets_test_private = np.load(join(private_directory, 'text_test_private_tweets.npy'))\n",
    "\n",
    "idx_to_word = {i: w for w, i in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below Cell contains all the Functions used in the Notebook which help us in pre processing the Data.\n",
    "\n",
    "* remove_punct : This function is used to remove all the punctuations from the text\n",
    "* tokenization : This function is used to split longer strings of data into smaller strings\n",
    "* load_data : In this fucntion, we are converting the data which we obtained into readable format, in this case into dataframes. We also remove jargon values from the text such as start, end and user.\n",
    "* stemming : We use stemming to remove the affixes from a word and obtain the root word\n",
    "* lemmatizer : We use lemmatization to capture canonical forms based on a word's lemma. Eg : better → good\n",
    "* convert_emojis : We use the convert emojis fucntion to convert the emojis into the their meaning. eg : a sushi emoji will be changed to the word sushi.\n",
    "* convert_emoticons : We use the convert emojis fucntion to convert the emojis into the their meaning. eg : a happy emoji will be changed to the text happy.\n",
    "* preprocessing : The preprocessing function is used to preprocess the text. In this function we call the others functions too which will help us in preprocessing the data. We remove punctuations, we remove stop words, emojis and emoticons. We also sem and lemmatize the data. we change the data into lower case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "def load_data(tweet, labels):\n",
    "    loaded_df = pd.DataFrame()\n",
    "    counter=0\n",
    "    new_list=[]\n",
    "    sentiments=[]\n",
    "    for j in range(len(tweet)):\n",
    "        end=0\n",
    "        dummy_string=\"\"\n",
    "        word_list=[]\n",
    "        for count in range(50):\n",
    "            word_list.append(idx_to_word[tweet[counter][count]])\n",
    "        for word in word_list:\n",
    "            if \"<END>\" in word:\n",
    "                end=1\n",
    "            elif \"<START>\" in word:\n",
    "                continue;\n",
    "            elif \"<user>\" in word:\n",
    "                continue;\n",
    "            elif(end==0):\n",
    "                dummy_string=dummy_string + word + \" \"\n",
    "        new_list.append(dummy_string)\n",
    "        if (len(labels) > 0):\n",
    "            sentiments.append(labels[counter])\n",
    "        counter += 1\n",
    "    loaded_df[\"Text\"]=new_list\n",
    "    if (len(sentiments) > 0):\n",
    "        loaded_df[\"Label\"]=sentiments\n",
    "    return loaded_df\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "# Converting emojis to words\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Converting emoticons to words    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "def preprocessing(df):\n",
    "    df.dropna(inplace=True)\n",
    "    df= df.apply(lambda x: convert_emojis(x))\n",
    "    df= df.apply(lambda x: convert_emoticons(x))\n",
    "    df = df.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    stop = stopwords.words('english')\n",
    "    df = df.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    df = df.apply(lambda x: remove_punct(x))\n",
    "    df = df.apply(lambda x: tokenization(x.lower()))\n",
    "    df = df.apply(lambda x: stemming(x))\n",
    "    df = df.apply(lambda x: lemmatizer(x))\n",
    "    \n",
    "    for i in range(0, len(df)):\n",
    "        processed_feature = re.sub(r'\\W', ' ', str(df[i]))\n",
    "        processed_feature = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "        processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "        df[i] = processed_feature.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the Training Dataset into a dataframe and then preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>make fuck irat jesu nobodi call ppl like haji...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lol adam bull fake outrag</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pas away earli morn fast furiou style car cra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lol wow gonna say realli haha seen chri nah d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>need bentobox sushi date ricebal spaghetti ol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0   make fuck irat jesu nobodi call ppl like haji...      0\n",
       "1                         lol adam bull fake outrag       0\n",
       "2   pas away earli morn fast furiou style car cra...      0\n",
       "3   lol wow gonna say realli haha seen chri nah d...      0\n",
       "4   need bentobox sushi date ricebal spaghetti ol...      0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = load_data(tweets_train,labels_train)\n",
    "training_dataframe[\"Text\"] = preprocessing(training_dataframe[\"Text\"])\n",
    "training_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the Validation Dataset into a dataframe and then preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fume hijack money move full back poutingfac</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nightmar dream freedom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnn realli need get busi number second fallen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kikm horni kik nude girl wearyfac horni snap</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fuck tag pictur famili first cut number year ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0       fume hijack money move full back poutingfac       0\n",
       "1                            nightmar dream freedom       0\n",
       "2   cnn realli need get busi number second fallen...      0\n",
       "3      kikm horni kik nude girl wearyfac horni snap       0\n",
       "4   fuck tag pictur famili first cut number year ...      0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataframe = load_data(tweets_val, labels_val)\n",
    "validation_dataframe[\"Text\"] = preprocessing(validation_dataframe[\"Text\"])\n",
    "validation_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the Public Dataset into a dataframe and then preprocessing it.\n",
    "\n",
    "The public dataset doesnot contain labels and are load_data function requires the label_list parameter. Hence, we are sending an empty list to fulfil the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>omg mother daughter dull ni move dad worri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happi birthday repeat miss excit back florida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ever cri middl bomb rest someon woke emerg sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mental suffer worthless pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>courag driver shot bu show courag natur scare...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0        omg mother daughter dull ni move dad worri \n",
       "1   happi birthday repeat miss excit back florida...\n",
       "2   ever cri middl bomb rest someon woke emerg sl...\n",
       "3                      mental suffer worthless pain \n",
       "4   courag driver shot bu show courag natur scare..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = []\n",
    "public_dataframe = load_data(tweets_test_public,label_list)\n",
    "public_dataframe[\"Text\"] = preprocessing(public_dataframe[\"Text\"])\n",
    "public_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the Private Dataset into a dataframe and then preprocessing it.\n",
    "\n",
    "The private dataset doesnot contain labels and are load_data function requires the label_list parameter. Hence, we are sending an empty list to fulfil the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whatev decid make sure make happi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accept challeng liter even feel exhilar victo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roommat okay spell autocorrect terribl firstw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cute atsu probabl shi photo cherri help uwu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rooney fuck untouch fuck dread depay look dec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0                 whatev decid make sure make happi \n",
       "1   accept challeng liter even feel exhilar victo...\n",
       "2   roommat okay spell autocorrect terribl firstw...\n",
       "3       cute atsu probabl shi photo cherri help uwu \n",
       "4   rooney fuck untouch fuck dread depay look dec..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = []\n",
    "private_dataframe = load_data(tweets_test_private,label_list)\n",
    "private_dataframe[\"Text\"] = preprocessing(private_dataframe[\"Text\"])\n",
    "private_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining the dataframes to create a bigger dataset for input which will have more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>make fuck irat jesu nobodi call ppl like haji...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lol adam bull fake outrag</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pas away earli morn fast furiou style car cra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lol wow gonna say realli haha seen chri nah d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>need bentobox sushi date ricebal spaghetti ol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0   make fuck irat jesu nobodi call ppl like haji...      0\n",
       "1                         lol adam bull fake outrag       0\n",
       "2   pas away earli morn fast furiou style car cra...      0\n",
       "3   lol wow gonna say realli haha seen chri nah d...      0\n",
       "4   need bentobox sushi date ricebal spaghetti ol...      0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataframe = training_dataframe.append(validation_dataframe, ignore_index=True)\n",
    "combined_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data to be passed for the models\n",
    "\n",
    "* Converting the text values to vectors. \n",
    "    * We use count vector to vectorize the data.The CountVectorizer function provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. \n",
    "* Converting the Labels to numpy to match the vectorized input variable using get_dummies\n",
    "* Setting y_test and y_train and also the values for the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(training_dataframe[\"Text\"].values)\n",
    "\n",
    "X_train = vectorizer.transform(training_dataframe[\"Text\"].values)\n",
    "X_test  = vectorizer.transform(validation_dataframe[\"Text\"].values)\n",
    "y_train = training_dataframe[\"Label\"]\n",
    "y_test  = validation_dataframe[\"Label\"]\n",
    "y_np = pd.get_dummies(validation_dataframe['Label']).values\n",
    "x_np = pd.get_dummies(training_dataframe['Label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Values for combined Dataset\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(combined_dataframe[\"Text\"].values)\n",
    "\n",
    "X_combined = vectorizer.transform(combined_dataframe[\"Text\"].values)\n",
    "X_test  = vectorizer.transform(validation_dataframe[\"Text\"].values)\n",
    "y_combined = combined_dataframe[\"Label\"]\n",
    "c_np=pd.get_dummies(combined_dataframe['Label']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional ML Model:\n",
    "\n",
    "We have used 3 conventional models to find out the best accuracy out of which Random forest had the highest Accuracy and we chose that model as the best models.\n",
    "\n",
    "**For all the conventional models, we have fitted the model using the combined dataframe as it will provide more values and more features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Logistic Regression model gave us an accuracy of 56.30 which is lower than the Random Forest Model. We used 2 default parameters which was multi_class which we kept is as 'auto' and the solver parameter where we used liblinear as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  56.3013698630137\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(multi_class=\"auto\",solver=\"liblinear\")\n",
    "classifier.fit(X_combined, y_combined)\n",
    "logistic = classifier.predict(X_test)\n",
    "accuracy_LR = accuracy_score(logistic, y_test)*100\n",
    "print(\"Logistic Regression Accuracy Score -> \", accuracy_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Naive Bayes model gave us an accuracy of 52.60 which is comparatively very low as compared to the other 2 models used. We used Multinomial Naive Bayes because it is very efficient of text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  52.602739726027394\n"
     ]
    }
   ],
   "source": [
    "naive = naive_bayes.MultinomialNB()\n",
    "naive.fit(X_combined, y_combined)\n",
    "predictions_NB = naive.predict(X_test)\n",
    "accuracy_NB =  accuracy_score(predictions_NB, y_test)*100\n",
    "print(\"Naive Bayes Accuracy Score -> \", accuracy_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Random Forest Model gave us an accuracy of 60.95 which was the highest amongst all the 3 models . We use the n_estimators parameter which shows the number of trees to be used in the forest, in this case its 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy Score ->  60.95890410958904\n"
     ]
    }
   ],
   "source": [
    "RForest = RandomForestClassifier(n_estimators=1000)\n",
    "RForest.fit(X_combined, y_combined)\n",
    "predictions_RForest = RForest.predict(X_test)\n",
    "accuracy_RF = accuracy_score(predictions_RForest, y_test)*100\n",
    "print(\"Random Forest Accuracy Score -> \", accuracy_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the Conventional ML Model\n",
    "\n",
    "From the above accuracies, we can see that Random forest is the best model as it has the highest accuracy and hence it is the final model and used for prediction. This could be because Random forest creates multiples trees in the forest. n_estimators is the hyperparameter used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the CSV file for the predictions made by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame()\n",
    "output_df[\"ID\"] = [x for x in range(0, len(validation_dataframe))]\n",
    "output_df[\"Prediction\"] = predictions_RForest\n",
    "output_df.to_csv(\"ConventionalModel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model\n",
    "\n",
    "The final model that produced the best-performing predictions for the Kaggle submission was the CNN model with accuracy 60%. The first input dimension or feature for the first dense layer was the dimension of the dataset and the output was 1000 which inturn was the input the following dataset and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters used for the CNN model are as follows:\n",
    "* Dense is a standard layer type that is used in many cases for neural networks.\n",
    "* Relu that is rectified linear activation function returns the value provided as input directly, when training a neural network.\n",
    "* add function is used to add layers to our model.\n",
    "* Sequential model is used as the layers are stacked sequentially that is input and output layer with their respective shapes.\n",
    "* We use kernel regularizers to avoid overfitting and smoothen the regression line. We use L2 type of kernel regularization.\n",
    "* As the output layer is a multiclass classification problem \"softmax\" has been used as output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=9812, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1000, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  \"\"\"\n",
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1000, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  \n",
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1000, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  import sys\n",
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=1000, activation=\"relu\", kernel_regularizer=<keras.reg..., units=1000)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "input_dim = X_combined.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(output_dim=1000, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(layers.Dense(output_dim=1000, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(layers.Dense(output_dim=1000, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(layers.Dense(output_dim=1000, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(layers.Dense(output_dim=1000, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(layers.Dense(4, activation='softmax',kernel_regularizer=regularizers.l2(0.0001)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Compile the model in the below paramters:\n",
    "* The optimizer controls the learning rate. We have used Adam as the optimizer. The adam optimizer adjusts the learning rate throughout training.\n",
    "* We have used categorical_crossentropy for the loss fucntion. The lower the loss score means the model is a better performer\n",
    "* We use accuracy as the metric to make it easier for us to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1000)              9813000   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 4004      \n",
      "=================================================================\n",
      "Total params: 13,821,004\n",
      "Trainable params: 13,821,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "             optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model\n",
    "\n",
    "We use the following paramters to compile our model:\n",
    "* We use the training paramters of the combined Dataset\n",
    "* The number of epochs represent the number of times the model will cycle through the data. We tried multiple epochs and a saturation was reached between 10-20 epochs. Hence we have used 10 epochs.\n",
    "* For the validation data, we use the validation dataset.\n",
    "* We have set the batch_size to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8558 samples, validate on 1460 samples\n",
      "Epoch 1/10\n",
      "8558/8558 [==============================] - 13s 1ms/step - loss: 1.4319 - accuracy: 0.5668 - val_loss: 1.3251 - val_accuracy: 0.5404\n",
      "Epoch 2/10\n",
      "8558/8558 [==============================] - 12s 1ms/step - loss: 0.6449 - accuracy: 0.8625 - val_loss: 1.0657 - val_accuracy: 0.5925\n",
      "Epoch 3/10\n",
      "8558/8558 [==============================] - 12s 1ms/step - loss: 0.4620 - accuracy: 0.8878 - val_loss: 0.9382 - val_accuracy: 0.6021\n",
      "Epoch 4/10\n",
      "8558/8558 [==============================] - 12s 1ms/step - loss: 0.3863 - accuracy: 0.8962 - val_loss: 0.8794 - val_accuracy: 0.6048\n",
      "Epoch 5/10\n",
      "8558/8558 [==============================] - 13s 1ms/step - loss: 0.3508 - accuracy: 0.9004 - val_loss: 0.8544 - val_accuracy: 0.6068\n",
      "Epoch 6/10\n",
      "8558/8558 [==============================] - 12s 1ms/step - loss: 0.3257 - accuracy: 0.9013 - val_loss: 0.8168 - val_accuracy: 0.6082\n",
      "Epoch 7/10\n",
      "8558/8558 [==============================] - 11s 1ms/step - loss: 0.3112 - accuracy: 0.9024 - val_loss: 0.7990 - val_accuracy: 0.6075\n",
      "Epoch 8/10\n",
      "8558/8558 [==============================] - 11s 1ms/step - loss: 0.2960 - accuracy: 0.9004 - val_loss: 0.7864 - val_accuracy: 0.6068\n",
      "Epoch 9/10\n",
      "8558/8558 [==============================] - 11s 1ms/step - loss: 0.2917 - accuracy: 0.9004 - val_loss: 0.7867 - val_accuracy: 0.6048\n",
      "Epoch 10/10\n",
      "8558/8558 [==============================] - 12s 1ms/step - loss: 0.2970 - accuracy: 0.8966 - val_loss: 0.7871 - val_accuracy: 0.6055\n",
      "Testing Accuracy:  0.6055\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_combined, c_np,\n",
    "                    epochs=10,\n",
    "                   verbose=True,\n",
    "                 validation_data=(X_test, y_np),\n",
    "                 batch_size=256)\n",
    "loss, accuracy = model.evaluate(X_test, y_np, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on the Deep Learning Model\n",
    "\n",
    "The deep learning model had the accuracy of 0.605 on the validation data set and had the hyperparameters Dense, Relu, Kernel regulazier and softmax as the output layer which are explained in detail above while builing the model.\n",
    "\n",
    "We also tried a model with a few different parameters such as Dropout, LSTM, GlobalMaxPooling1D but the accuracy was significantly low and around 52%\n",
    "\n",
    "The Deep Learning accuracy on the private dataset was 0.64082 where as it was 0.56865 on the public dataset and it was 0.6055 on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Model Performance and Implementation\n",
    "\n",
    "* By observing the models, random forest gave the best accuracy out of the conventional models and also was slightly higher than the CNN model on the validation dataset.\n",
    "* For validation Dataset, random forest gave the highest accuracy[60.9589] as compared to the deep learning CNN model which gave the accuracy of 0.6055.\n",
    "* On the public dataset, CNN model had the highest accuracy which was 0.56865 while the random forest model gave 0.56028 which was slightly lower.\n",
    "* On the private dataset too, the CNN model had a higher accuracy which was 0.64129 as compared to the accuracy of the Random Forest model.\n",
    "* The accuracy score on the Private dataset was 0.64129 which was around 7% more than it had on the public dataset and around 3% more than it was on the validation dataset. \n",
    "* The private dataset accuracy was 0.64129 which ranked 25th on the private dataset and was only 0.82% lower than the 2nd rank. \n",
    "* Hence, CNN was the best machine learning model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
